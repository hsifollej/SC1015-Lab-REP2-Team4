{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "990d26bc4b3147dd96dfe274a8fa3fa8",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# SC1015 Lab REP2 Team 4 DSAI Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bc32f5645dd24ba2addfbe68eb2e21d2",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Sricharan Balasubramanian, Travis Tan Hai Shuo, Joanna Wu Haoyue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a6979e226cf64331a27df7dba718b7e4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In light of our upcoming exchange in UC Berkeley, the 3 of us are excited to go on many roadtrips to explore the beautiful state of California. However, we are concerned about the road safety there and want to know which streets are most prone to accidents, so we can be more cautious.\n",
    "\n",
    "Upon finding the Kaggle dataset of \"US Accidents\" that contained data on road accidents all across US, we decided to scope down this dataset to only accidents that happened in the state of California. Coincidentally, with a quick visualisation of the dataset, we realised California is the state with the highest number of road accidents, which inspired a greater goal and use case.\n",
    "\n",
    "Thus, in this project we will be using this dataset to find out the times of day or night where particular streets are most accident prone. We also want to find correlation between weather conditions and accidents. With these insights, emergency services like firefighters and hospitals can have an early warning system to help them allocate rescue resources optimally, reducing the fatality of road accidents. Additionally, these insights can be integrated into GPS systems to alert drivers when they are driving on certain streets at accident-prone times or weather conditions, minimising road accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install contextily\n",
    "!pip install shapely\n",
    "!pip install folium\n",
    "!pip install imbalanced-learn xgboost\n",
    "!pip install pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "58a5301945204bf8a1d43b5f9f37db60",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1397,
    "execution_start": 1745140617453,
    "source_hash": "ef9a1401"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8db2d221833446f4bf9546e5e092325a",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 16468,
    "execution_start": 1745140618910,
    "source_hash": "1cd449f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('US_Accidents_March23_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6b12118ef3e448339c897735f318ee73",
    "deepnote_cell_type": "code",
    "deepnote_table_invalid": false,
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "cellFormattingRules": [],
     "columnDisplayNames": [],
     "columnOrder": [
      "ID",
      "Source",
      "Severity",
      "Start_Time",
      "End_Time",
      "Start_Lat",
      "Start_Lng",
      "End_Lat",
      "End_Lng",
      "Distance(mi)",
      "Description",
      "Street",
      "City",
      "County",
      "State",
      "Zipcode",
      "Country",
      "Timezone",
      "Airport_Code",
      "Weather_Timestamp",
      "Temperature(F)",
      "Wind_Chill(F)",
      "Humidity(%)",
      "Pressure(in)",
      "Visibility(mi)",
      "Wind_Direction",
      "Wind_Speed(mph)",
      "Precipitation(in)",
      "Weather_Condition",
      "Amenity",
      "Bump",
      "Crossing",
      "Give_Way",
      "Junction",
      "No_Exit",
      "Railway",
      "Roundabout",
      "Station",
      "Stop",
      "Traffic_Calming",
      "Traffic_Signal",
      "Turning_Loop",
      "Sunrise_Sunset",
      "Civil_Twilight",
      "Nautical_Twilight",
      "Astronomical_Twilight"
     ],
     "conditionalFilters": [],
     "filters": [],
     "hiddenColumnIds": [],
     "pageIndex": 49,
     "pageSize": 10,
     "sortBy": [],
     "wrappedTextColumnIds": []
    },
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 20,
    "execution_start": 1745140636921,
    "source_hash": "a80e9227"
   },
   "outputs": [],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "373873e16229437ba14e9856f6cc425a",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 0,
    "execution_start": 1745140636991,
    "source_hash": "de1e323c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "823bfba483614be495bfabb4d2f10519",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 2794,
    "execution_start": 1745140637080,
    "source_hash": "ae6b491"
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0b58c772247f4c22b353a81a30430e7b",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "cellFormattingRules": [],
     "columnDisplayNames": [],
     "columnOrder": [
      "ID",
      "Source",
      "Severity",
      "Start_Time",
      "End_Time",
      "Start_Lat",
      "Start_Lng",
      "End_Lat",
      "End_Lng",
      "Distance(mi)",
      "Description",
      "Street",
      "City",
      "County",
      "State",
      "Zipcode",
      "Country",
      "Timezone",
      "Airport_Code",
      "Weather_Timestamp",
      "Temperature(F)",
      "Wind_Chill(F)",
      "Humidity(%)",
      "Pressure(in)",
      "Visibility(mi)",
      "Wind_Direction",
      "Wind_Speed(mph)",
      "Precipitation(in)",
      "Weather_Condition",
      "Amenity",
      "Bump",
      "Crossing",
      "Give_Way",
      "Junction",
      "No_Exit",
      "Railway",
      "Roundabout",
      "Station",
      "Stop",
      "Traffic_Calming",
      "Traffic_Signal",
      "Turning_Loop",
      "Sunrise_Sunset",
      "Civil_Twilight",
      "Nautical_Twilight",
      "Astronomical_Twilight"
     ],
     "conditionalFilters": [],
     "filters": [],
     "hiddenColumnIds": [],
     "pageIndex": 0,
     "pageSize": 10,
     "sortBy": [],
     "wrappedTextColumnIds": []
    },
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 6107,
    "execution_start": 1745140640999,
    "source_hash": "b1cecf2d"
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "36099012222d4cf1bea6d2cc7e21c08d",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8690803a35a24db5af0ac065d6dc66f2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We will prepare the data by performing data cleaning to suit our problem statement. This begins with feature reduction on the California dataset, to improve model performance and reduce complexity. \n",
    "\n",
    "We will eliminate columns that are not necessary for our analysis, such as columns with only 1 unique value (e.g. 'country' and 'state'columns, because all accidents happen in the same country and state). Likewise, for Turning_Loop where every value is False, there is no value added with purely negative data. We will also eliminate columns that are irrelevant to the problem, like timezone and the various twilights, as they are just alternative ways to tell time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2f6d1338f00449cb99395c1d791ada48",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We will also analyse the duration of accident using (end_time - start_time) to see if end_time can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "091d52dd385a46dca22f6afe2eaf0405",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1079,
    "execution_start": 1745140647161,
    "source_hash": "6293eb34"
   },
   "outputs": [],
   "source": [
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "80585bc56ba2406ab914be152170e746",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 0,
    "execution_start": 1745140648301,
    "source_hash": "d6d2af2c"
   },
   "outputs": [],
   "source": [
    "df['Duration'] = df['End_Time'] - df['Start_Time']\n",
    "df['Duration_Minutes'] = df['Duration'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5e09bfc217d846c8a642f6e60a8cb5ec",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Since some of the accident durations are days, we will remove them from this visualisation as the focus of our project is not on how accidents affect traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fa45cec01b5140e185289654fb6db317",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 4498,
    "execution_start": 1745140648351,
    "source_hash": "94b76b1"
   },
   "outputs": [],
   "source": [
    "filtered_df = df[df['Duration_Minutes'] <= 300]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sb.histplot(filtered_df['Duration_Minutes'], bins=50, kde=True)\n",
    "plt.title(\"Filtered Distribution of Accident Durations (<= 300 mins)\")\n",
    "plt.xlabel(\"Duration (minutes)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b96847d3ecb4547a308b9a70c780bfb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We observe that most accident durations is about 25 minutes, which we deem to be too short to be relevant to our dataset and problem statement. Hence, we justify removing End_Time (and therefore End_Lat, End_Lng too) from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6d8502e73bdf405e8a5ff1aec03c1ff7",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 84,
    "execution_start": 1745140652901,
    "source_hash": "67d5fa93"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'ID', 'Source', 'End_Time','End_Lat', 'End_Lng', 'Country', 'State', 'Timezone', 'Turning_Loop','Airport_Code', 'Weather_Timestamp',\n",
    "    'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Duration', 'Duration_Minutes'\n",
    "]\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b94c6e9f17044fe7bfa607ca0b82c0ca",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1,
    "execution_start": 1745140653040,
    "source_hash": "de1e323c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "10a9d02092b244e88f93fda2190d3ccb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We will now convert this dataset's features into their appropriate types, making it easier to handle the data and help the ML models better understand the features. This is important for datetime and categorical features.\n",
    "\n",
    "We realise an important step is to split the Start_Time into date and time separately, because our problem statement considers time to be more important than date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0fa1c3ec62bc4513ae7a892ba9429e1c",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 3065,
    "execution_start": 1745140653090,
    "source_hash": "4e83d7b1"
   },
   "outputs": [],
   "source": [
    "# Convert object columns to category\n",
    "object_cols = df.select_dtypes(include=['object']).columns\n",
    "df[object_cols] = df[object_cols].astype('category')\n",
    "\n",
    "# Specifically convert 'Description' to string\n",
    "df[\"Description\"] = df[\"Description\"].astype('string')\n",
    "\n",
    "# Start_Time has already been converted into datetime earlier, but here we will split it into date and time separately\n",
    "\n",
    "# Split Start_Time into separate features\n",
    "df['Start_Date'] = df['Start_Time'].dt.date\n",
    "df['Start_Hour'] = df['Start_Time'].dt.hour\n",
    "df['Start_Minute'] = df['Start_Time'].dt.minute\n",
    "df['Accident_Time'] = df['Start_Hour'] + df['Start_Minute'] / 60\n",
    "\n",
    "# Update column type lists AFTER cleaning\n",
    "datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['category']).columns.tolist()\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "string_cols = df.select_dtypes(include=['string']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d78cfcd9e9ff40319de7c4c0af514dc4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "These are the converted data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "be7af70fff354880bfd2f901b4d4bea2",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 58,
    "execution_start": 1745140656210,
    "source_hash": "de1e323c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38e59c32b9ff4a278a2947df83bed04e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "After feature reduction and keeping only columns we deemed as relevant, now we will handle any NULL values present in the dataset. Let's look at the spread of NULL values within the dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d45ab2f021ed41cd993f1eff6a4e4a36",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 120,
    "execution_start": 1745140656321,
    "source_hash": "a703a5a"
   },
   "outputs": [],
   "source": [
    "null_count = df.isnull().sum()\n",
    "\n",
    "total_rows = len(df)\n",
    "null_percentage = (null_count / total_rows) * 100\n",
    "\n",
    "null_df = pd.DataFrame({'Null Count': null_count, 'Null Percentage': null_percentage})\n",
    "print(null_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "df4d7a74e3524f4394657394e59ccf09",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We noticed approximately 10% of the dataset contains missing values in the Start_Time column, which might be a result of improper date time format that failed to convert, resulting in NaT (Not a time) NULL value replacing the data point. Since Start_Time is a critical variable used to derive several temporal features such as accident time, rows lacking this information are unsuitable for time-based analysis, but 10% is a considerable amount of data to be dropped.\n",
    "\n",
    "We decided to visualise the distribution of streeets across the dataset to observe if the rows with NULL accident time will skew the results heavily if removed. For example, if a street has disproportionately higher accidents with no time than ones recorded with time, then removing the NULL times would impact the ML results of that street. We will select the streets with the most number of accidents to plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "93376c526e544ffdb61230504fa92538",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 474,
    "execution_start": 1745140656491,
    "source_hash": "db81735d"
   },
   "outputs": [],
   "source": [
    "df_with_time = df[df['Start_Time'].notnull()]\n",
    "df_without_time = df[df['Start_Time'].isnull()]\n",
    "\n",
    "# Count accidents per street for each group\n",
    "street_counts_with_time = df_with_time['Street'].value_counts()\n",
    "street_counts_without_time = df_without_time['Street'].value_counts()\n",
    "\n",
    "# Combine counts into a DataFrame\n",
    "street_counts = pd.DataFrame({\n",
    "    'With Start_Time': street_counts_with_time,\n",
    "    'Without Start_Time': street_counts_without_time\n",
    "}).fillna(0)\n",
    "\n",
    "# Normalise to get proportions\n",
    "street_counts_norm = street_counts.div(street_counts.sum(axis=0), axis=1)\n",
    "\n",
    "# Select top N streets by total accidents\n",
    "top_n = 10\n",
    "top_streets = street_counts.sum(axis=1).nlargest(top_n).index\n",
    "street_counts_top = street_counts_norm.loc[top_streets]\n",
    "\n",
    "# Plot\n",
    "street_counts_top.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Proportion of Accidents by Street: With vs. Without Start_Time')\n",
    "plt.xlabel('Street')\n",
    "plt.ylabel('Proportion of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "189fef480dab4191a8eb346434c31e60",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Since the proportions of accidents across streets are similar between the two groups, dropping rows with missing Start_Time is unlikely to bias our analysis. We also believe that imputing the NULL times with an average time would not be the best way to handle the data, and thus decided on dropping rows with missing Start_Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "53daf206bc3b48fabe0d9eed08085efc",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 182,
    "execution_start": 1745140657011,
    "source_hash": "67b2a93"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['Start_Time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fc815b14b2604731b898fb797cfb1e82",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 133,
    "execution_start": 1745140657241,
    "source_hash": "de1e323c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "633e2ea477b84e4b8048f136e87421df",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "To address the other NULL values, they will be broken down into 3 groups: \n",
    "1. Crticial columns with low NULLs/NULL percentages: These rows will be dropped as the change is negligible but will greatly improve quality of dataset\n",
    "2. Numerical weather features (2-9% missing values): These rows will be filled with median values to reflect the central tendency of these weather conditions\n",
    "3. Columns with high NULLs/NULL percentages (Wind_Chill and Precipitation): Wind_Chill will be dropped as with such a high NULL percentage, it is often redundant and comes hand in hand with attribute from another column such as Temperature. \n",
    "\n",
    "Precipitation NULLs will be inputed with 0 but not dropped, because rain is an important weather condition. If precipiation were a factor of the accident, we assume it would have been documented. Thus, empty cells for precipiation implies rain was not a contributive factor and we assume NULL cells will be replaced with 0. Disclaimer: precipitation is often not measured precisely as snow may not be counted as Precipitation.\n",
    "\n",
    "We will ignore Description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2a1cc40734134090b3284c18166e8d81",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1108,
    "execution_start": 1745140657431,
    "source_hash": "5c36671b"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values BEFORE cleaning:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "# dealing with group 1\n",
    "df.dropna(subset=['Street', 'City'], inplace=True)\n",
    "\n",
    "# dealing with group 2\n",
    "weather_num_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "for col in weather_num_cols:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# handling missing Zipcode\n",
    "if 'Unknown' not in df['Zipcode'].cat.categories:\n",
    "    df['Zipcode'] = df['Zipcode'].cat.add_categories('Unknown')\n",
    "df['Zipcode'] = df['Zipcode'].fillna('Unknown')\n",
    "\n",
    "\n",
    "# Fill categorical/object-based weather columns with 'Unknown'\n",
    "if 'Wind_Direction' in df.columns and df['Wind_Direction'].dtype.name == 'category':\n",
    "    if 'Unknown' not in df['Wind_Direction'].cat.categories:\n",
    "        df['Wind_Direction'] = df['Wind_Direction'].cat.add_categories('Unknown')\n",
    "    df['Wind_Direction'] = df['Wind_Direction'].fillna('Unknown')\n",
    "\n",
    "if 'Weather_Condition' in df.columns and df['Weather_Condition'].dtype.name == 'category':\n",
    "    if 'Unknown' not in df['Weather_Condition'].cat.categories:\n",
    "        df['Weather_Condition'] = df['Weather_Condition'].cat.add_categories('Unknown')\n",
    "    df['Weather_Condition'] = df['Weather_Condition'].fillna('Unknown')\n",
    "\n",
    "df.drop(columns=['Wind_Chill(F)'], inplace=True) \n",
    "df['Precipitation(in)'].fillna(0, inplace=True)  # assumes most missing = no rain\n",
    "\n",
    "\n",
    "# Optional: Fill less important columns with default (for modeling)\n",
    "optional_bool_cols = df.select_dtypes(include='bool').columns\n",
    "df[optional_bool_cols] = df[optional_bool_cols].fillna(False)\n",
    "\n",
    "# Check again after cleaning\n",
    "print(\"\\nMissing values AFTER cleaning:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "36be788a06344a3cba4583af1df0a1c5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We have successfully handled NULL values in our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e9927013496c451cbfbe02343e2835f5",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Distribution Analysis & Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "368aec57de81407aa6d4aa8fcd2fcebf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now, looking at how some columns have many unique values (continuous numericial), we will normalise the data to remove outliers as they can skew data analysis and affect the reliability of ML models like KNN and clustering.\n",
    "\n",
    "We will begin with a visualisation of potential outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d946a3f6075d4dce85c44f381708c75d",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 2557,
    "execution_start": 1745140658600,
    "source_hash": "65119826"
   },
   "outputs": [],
   "source": [
    "def plot_numeric_features(df, numeric_features):\n",
    "    df[numeric_features] = df[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "    df[numeric_features] = df[numeric_features].fillna(0)\n",
    "    num_cols = len(numeric_features)\n",
    "    fig, axs = plt.subplots(num_cols, 2, figsize=(12, num_cols*6))\n",
    "\n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        # Histogram\n",
    "        axs[i, 0].hist(df[feature], bins=20, color='skyblue', edgecolor='black')\n",
    "        axs[i, 0].set_title(f'{feature} Histogram')\n",
    "        axs[i, 0].set_xlabel(feature)\n",
    "        axs[i, 0].set_ylabel('Frequency')\n",
    "\n",
    "        # Boxplot\n",
    "        axs[i, 1].boxplot(df[feature], vert=True)\n",
    "        axs[i, 1].set_title(f'{feature} Boxplot')\n",
    "        axs[i, 1].set_xlabel(feature)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "numeric_features = ['Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "                    'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "plot_numeric_features(df, numeric_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "61021256bceb44c1be7262fabb21776a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Outliers are detected in precipiation, wind_speed, visbility and distance. Possible explanations would be that most days go with 0 rain, and crashes often occur nearby. Either ways, the data can be normalised to minimise the skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0662e9989607477198c44a54ab052ea3",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1357,
    "execution_start": 1745140661210,
    "source_hash": "407fccc"
   },
   "outputs": [],
   "source": [
    "def plot_boolean_features(df, bool_features):\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_features = len(bool_features)\n",
    "    num_rows = num_features // 2 + num_features % 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create subplots with rectangular shape\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 30))\n",
    "\n",
    "    # Flatten the axes array to make it easier to iterate\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot boolean counts for specified features\n",
    "    for i, feature in enumerate(bool_features):\n",
    "        counts = df[feature].value_counts()\n",
    "        counts.plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'Boolean counts for {feature}')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(num_features, num_rows * num_cols):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_boolean_features(df, bool_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3f847d8623d485ca44e9a79c65530b0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In all the above graphs, false values are more frequent than true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1f8a478543834037ba91b3abfa92778e",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 0,
    "execution_start": 1745140662651,
    "source_hash": "942191f9"
   },
   "outputs": [],
   "source": [
    "for column in df[bool_cols].columns:\n",
    "    true_percentage = df[column].mean() * 100\n",
    "    false_percentage = 100 - true_percentage\n",
    "\n",
    "    print(column)\n",
    "    print(f\"Percentage of True: {true_percentage:.2f}%\")\n",
    "    print(f\"Percentage of False: {false_percentage:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "32e1c69730f24d9c9e44550845b86dd0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "Only Traffic_Signal, Junction, and Crossing have True values over 3% of all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "da86047f5b7c4d1180a1ba0b10fa0147",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1999,
    "execution_start": 1745140662700,
    "source_hash": "22747074"
   },
   "outputs": [],
   "source": [
    "def plot_top_categories(df, cat_features):\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_features = len(df[cat_features].columns)\n",
    "    num_rows = num_features // 2 + num_features % 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 50))\n",
    "\n",
    "    # Flatten the axes array to make it easier to iterate\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot top categories for each categorical feature\n",
    "    for i, column in enumerate(df[cat_features].columns):\n",
    "        top_categories = df[column].value_counts().nlargest(50)\n",
    "        top_categories.plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'Top categories for {column}')\n",
    "        axes[i].set_xlabel('Category')\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(num_features, num_rows * num_cols):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_categories(df, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7fd82f51f4c046e0a66b70cd24fd0db4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now we normalise and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f725aa4c4c1c4ce9bd7d51fcfbadea11",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 1044,
    "execution_start": 1745140664761,
    "source_hash": "23baffb5"
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns excluding 'Severity'\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_cols = [col for col in numeric_cols if col != 'Severity']  # Remove 'Severity' column\n",
    "\n",
    "# Normalize the remaining numeric columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Outlier removal\n",
    "Q1 = df[numeric_cols].quantile(0.25)\n",
    "Q3 = df[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out rows outside 1.5 * IQR range, excluding 'Severity' column\n",
    "df = df[~((df[numeric_cols] < (Q1 - 1.5 * IQR)) | (df[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e04550f8e5a4485b005e2a1fe468fab",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Correlation Analysis - Weather Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38fb5b45591a4b5a8d0746ef655ee10c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We will perform correlation analysis to identify the most correlated weather conditions with severity, and illustrate the relationships between various conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c49aee3ea4ec4d6a8ec1d2e699247b6c",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 233,
    "execution_start": 1745140696861,
    "source_hash": "59f49c38"
   },
   "outputs": [],
   "source": [
    "num_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Wind_Speed(mph)']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.heatmap(df[num_cols + ['Severity']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap with Severity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d6842e26f38e4f2f8e67ac147e27b167",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We will now perform feature merging to combine similar features. For example, we will combine Weather_Conditions as there are too many unique values inside, amany of which are overly-specified and semantically similar. This will reduce the complexity of our ML model, and improve its efficiency.\n",
    "\n",
    "To address this, we will map specific weather conditions into broader categories, and create binary flags for important weather conditions (e.g. Is_Windy, Is_Wet) to improve model interpretability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7d6b37bdd8314844aaaebc05d2274b78",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 673,
    "execution_start": 1745140774341,
    "source_hash": "4cd22d6d"
   },
   "outputs": [],
   "source": [
    "def simplify_weather(condition):\n",
    "    condition = condition.lower()\n",
    "    if 'rain' in condition or 'drizzle' in condition or 'shower' in condition:\n",
    "        return 'Rain'\n",
    "    elif 'snow' in condition or 'sleet' in condition:\n",
    "        return 'Snow'\n",
    "    elif 'fog' in condition or 'mist' in condition or 'haze' in condition or 'smoke' in condition:\n",
    "        return 'Fog'\n",
    "    elif 'storm' in condition or 'thunder' in condition or 'tstorm' in condition:\n",
    "        return 'Storm'\n",
    "    elif 'clear' in condition or 'sun' in condition:\n",
    "        return 'Clear'\n",
    "    elif 'cloud' in condition or 'overcast' in condition:\n",
    "        return 'Cloudy'\n",
    "    elif 'wind' in condition or 'breezy' in condition or 'gusty' in condition:\n",
    "        return 'Windy'\n",
    "    else:\n",
    "        return 'Other' # how many are classified under other?\n",
    "\n",
    "df['Weather_Simple'] = df['Weather_Condition'].astype(str).apply(simplify_weather)\n",
    "\n",
    "#to see other\n",
    "# Count how many rows were classified as 'Other'\n",
    "other_count = df[df['Weather_Simple'] == 'Other'].shape[0]\n",
    "print(f\"Number of rows classified as 'Other': {other_count}\")\n",
    "\n",
    "# See the unique Weather_Condition values that ended up as 'Other'\n",
    "other_conditions = df[df['Weather_Simple'] == 'Other']['Weather_Condition'].unique()\n",
    "print(\"Unique Weather_Condition values classified as 'Other':\")\n",
    "print(other_conditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4bad9109716549e8b2430b8cc9f82776",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Creating binary flags to capture each weather effect, which helps with readability and interpretability admist mixed weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d65c84b717334b4d8e918cb59ab35950",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 19,
    "execution_start": 1745140778858,
    "source_hash": "8cb1bbf3"
   },
   "outputs": [],
   "source": [
    "df['Is_Windy'] = df['Weather_Condition'].str.contains('Wind|Breezy|Gusty', case=False, na=False).astype(int)\n",
    "df['Is_Stormy'] = df['Weather_Condition'].str.contains('Storm|Thunder|Tstorm', case=False, na=False).astype(int)\n",
    "df['Is_Rainy'] = df['Weather_Condition'].str.contains('Rain|Drizzle|Shower', case=False, na=False).astype(int)\n",
    "df['Is_Foggy'] = df['Weather_Condition'].str.contains('Fog|Mist|Haze|Smoke', case=False, na=False).astype(int)\n",
    "df['Is_Snowy'] = df['Weather_Condition'].str.contains('Snow|Sleet', case=False, na=False).astype(int)\n",
    "df['Is_Clear'] = df['Weather_Condition'].str.contains('Clear|Sunny', case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d05074db483943d4ba6efc354144fca6",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 385,
    "execution_start": 1745141583463,
    "source_hash": "ae6b491"
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c7c2efce1184cdfbb90ec14539f0577",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "After normalisation and outlier removal, we realised Visibility (mi) and Precipitation (in) have been reduced to only 1 unique value, which does not provide any valuable insights. Hence, these 2 columns will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4defd5be958247749873082aa651743f",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 67,
    "execution_start": 1745141792212,
    "source_hash": "b7d9413b"
   },
   "outputs": [],
   "source": [
    "features_to_drop = [\n",
    "    'Visibility(mi)', 'Precipitation(in)'\n",
    "]\n",
    "df.drop(columns=features_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91f52a7d917a4b04991ca68215b3670b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Thus, the data has been properly processed through feature reduction, feature merging, NULL-value and outlier handling. Let's create the new DataFrame to be used for the EDA and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "43d31d0604664e2e8bd53d7a1ccf7ba7",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 201,
    "execution_start": 1745141804262,
    "source_hash": "c61bf381"
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b58e49ee08f34fd5a2ad1ef7554f5bdb",
    "deepnote_cell_type": "code",
    "execution_context_id": "e3717ca1-f937-408b-aacc-d35648a9036c",
    "execution_millis": 124,
    "execution_start": 1745141805338,
    "source_hash": "dcea1c60"
   },
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dfe01f4164b3429abb3339ef633a2879",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0b4ba58facb9459596c7fb2d3244d368",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "We have successfully cleaned our data and will now perform exploratory data analysis. We have a few hypotheses that we wish to explore, and will do them sequentially to validate potential trends, and subsequently feed into our ML algorithms for predictions. The items we wish to explore are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8ceb5a21927046e48cae98e0aace8848",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "1. Which regions in California have the most accidents, and which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5d4c926c172e4171b876b08fd8221688",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "2. What trends can we identify in the frequency of accidents against the time of the day, and the day of the week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5db295f3461e4537bb84ab467199902b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "3. Do accidents occur in all weather conditions or only in certain weather conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5782155e691b474e9a5f3f767d25c12c",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 504,
    "execution_start": 1745137288983,
    "source_hash": "1d505572"
   },
   "outputs": [],
   "source": [
    "df_cleaned.head(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "882e10da7ab446b2a24835fafba04894",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5a309eb82682432bbe0d522eeab6249a",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "We first start with a general plot of the entire California map with the accidents displayed, to be able to pick up some general trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c0cdaea1a31749958e9a95dfae849b16",
    "deepnote_cell_type": "code",
    "deepnote_output_height_limit_disabled": true,
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 20878,
    "execution_start": 1745137289933,
    "source_hash": "bc8979b8"
   },
   "outputs": [],
   "source": [
    "# Set plotting styles\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# sample for general trend\n",
    "sample_df = df_cleaned[['Start_Lat', 'Start_Lng']].dropna().sample(500000, random_state=42)\n",
    "\n",
    "# convert to GeoDataFrame in WGS84\n",
    "geometry = [Point(xy) for xy in zip(sample_df['Start_Lng'], sample_df['Start_Lat'])]\n",
    "gdf = gpd.GeoDataFrame(sample_df, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# project to Web Mercator\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# plot the accidents\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "gdf.plot(ax=ax, markersize=1, alpha=0.05, color='red')\n",
    "\n",
    "# after plotting gdf and adding basemap, get the map bounds\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "\n",
    "# add basemap and format\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "ax.set_title('Accident Density Map: California', fontsize=14)\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "41722c6320564050a6926e0c5908a659",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "It is clear that the accidents are not distributed homogeneously distributed in California. We see that they are concentrated in specific regions. For example, Los Angeles and the Bay Area show clear spikes. Moreover, we see accidents along lines through the state, probably indicating long roads. We confirm this with further visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d50deef061264616a32ec70e6c60a0a9",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 338,
    "execution_start": 1745137310863,
    "source_hash": "c02a20d8"
   },
   "outputs": [],
   "source": [
    "# get top 20 cities by accident count and turn it into a proper DataFrame\n",
    "top_cities_df = df_cleaned['City'].value_counts().head(20).reset_index()\n",
    "top_cities_df.columns = ['City', 'Accident_Count']\n",
    "\n",
    "top_cities_df = top_cities_df.sort_values('Accident_Count', ascending=True)\n",
    "\n",
    "ax = top_cities_df.plot(kind='barh', x='City', y='Accident_Count', legend=False, color='teal', figsize=(12, 6))\n",
    "\n",
    "# annotate each bar with the exact accident count\n",
    "for index, value in enumerate(top_cities_df['Accident_Count']):\n",
    "    ax.text(value, index, str(value), va='center', ha='left', color='black', fontsize=12)\n",
    "\n",
    "plt.title(\"Top 20 Cities with Most Accidents\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.ylabel(\"City\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d642d62b459c4f8bb8616d88067ebfb8",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "The city distribution shows clear imbalance in distributions, and this can be used as a foundation for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6cd930033f5a4bad96f56792d6f4ba0d",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 313,
    "execution_start": 1745137311254,
    "source_hash": "4515c43f"
   },
   "outputs": [],
   "source": [
    "# get top 20 streets by accident count and turn it into a proper DataFrame\n",
    "top_streets_df = df_cleaned['Street'].value_counts().head(20).reset_index()\n",
    "top_streets_df.columns = ['Street', 'Accident_Count']\n",
    "\n",
    "# sort by 'Accident_Count' in descending order\n",
    "top_streets_df = top_streets_df.sort_values('Accident_Count', ascending=True)\n",
    "\n",
    "# slot the bar chart (in ascending order, so the highest will be at the top)\n",
    "ax = top_streets_df.plot(kind='barh', x='Street', y='Accident_Count', legend=False, color='salmon', figsize=(12, 6))\n",
    "\n",
    "# annotate each bar with the street name and the exact accident count\n",
    "for index, value in enumerate(top_streets_df['Accident_Count']):\n",
    "    street_name = top_streets_df['Street'].iloc[index]\n",
    "    ax.text(value, index, f'{street_name}: {value}', va='center', ha='left', color='black', fontsize=12)\n",
    "\n",
    "plt.title(\"Top 20 Streets with Most Accidents\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.ylabel(\"Street\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7820514b56834cae9af751be4189875e",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "The split by street is a bit more homogeneous, and this can potentially be a weakness as there might be very high cardinality, inhibiting trend detection in our model. We have 60000 streets, and this will pose a problem in our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "04f5417c4dba409eb8004d6fc1a2c8ca",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 1633,
    "execution_start": 1745137311624,
    "source_hash": "4c436e90"
   },
   "outputs": [],
   "source": [
    "# plot: accidents per hour (ensure hour is 023, sorted)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.countplot(\n",
    "    x='Start_Hour',\n",
    "    data=df_cleaned,\n",
    "    order=sorted(df_cleaned['Start_Hour'].unique()),\n",
    "    palette='flare'\n",
    ")\n",
    "plt.title(\"Accidents per Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.xticks(ticks=range(24), labels=[str(i) for i in range(24)])  # force 023 x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4f7c485ceb514956a72d1125f1329ce5",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "We see a local spike between 7AM and 9AM, and a global spike between 4PM and 7PM. These are peak hours, and clearly this is another dimension for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "589c0f52742e4b54bb170e91ac2eac37",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "89a454a33a7d4df2b087c0b4ccf81460",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 1092,
    "execution_start": 1745137313303,
    "source_hash": "40bbeea5"
   },
   "outputs": [],
   "source": [
    "# extract day of week from Start_Time\n",
    "df_cleaned['Day_of_Week'] = df_cleaned['Start_Time'].dt.day_name()\n",
    "\n",
    "# plot: Accidents by Day of the Week\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sb.countplot(\n",
    "    x='Day_of_Week',\n",
    "    data=df_cleaned,\n",
    "    order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    palette='crest'\n",
    ")\n",
    "plt.title(\"Accidents by Day of the Week\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "56436ea66f374bb88ce0d8034c57e6a7",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "We also see a clear distinction between weekdays and weekends. A specific spike on Fridays, possibly an indicator of more haphazard driving. However, since we are trying to identify trends across both time and location, we will cross validate these trends.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "caa357058c82425d9d45220902697e84",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 757,
    "execution_start": 1745137314453,
    "source_hash": "c189656b"
   },
   "outputs": [],
   "source": [
    "# group by Street and Start_Hour and calculate the count of accidents\n",
    "time_location_street = df.groupby(['Street', 'Start_Hour']).size().unstack(fill_value=0)\n",
    "\n",
    "# calculate the total accidents in each street\n",
    "street_totals = time_location_street.sum(axis=1)\n",
    "\n",
    "# normalize the counts to proportions by dividing by the total accidents in each street\n",
    "time_location_proportion_street = time_location_street.div(street_totals, axis=0)\n",
    "\n",
    "# use top 20 streets\n",
    "top20_streets = df['Street'].value_counts().head(20).index\n",
    "sb.heatmap(time_location_proportion_street.loc[top20_streets], cmap=\"YlOrRd\", annot=False)  # Remove annotations\n",
    "plt.title(\"Proportion of Accidents by Hour in Top 20 Streets\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Street\")\n",
    "plt.xticks(ticks=range(0, 24), labels=[str(i) for i in range(24)])  # Ensure hour ticks are 0-23\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ceb78f84fc0b468e8afad263999cc5ce",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 401,
    "execution_start": 1745137315263,
    "source_hash": "d7604222"
   },
   "outputs": [],
   "source": [
    "# group by City and Start_Hour and calculate the count of accidents\n",
    "time_location = df.groupby(['City', 'Start_Hour']).size().unstack(fill_value=0)\n",
    "\n",
    "# calculate the total accidents in each city\n",
    "city_totals = time_location.sum(axis=1)\n",
    "\n",
    "# normalize the counts to proportions by dividing by the total accidents in each city\n",
    "time_location_proportion = time_location.div(city_totals, axis=0)\n",
    "\n",
    "# use top 20 cities\n",
    "top20 = df['City'].value_counts().head(20).index\n",
    "sb.heatmap(time_location_proportion.loc[top20], cmap=\"YlOrRd\", annot=False)  # Remove annotations\n",
    "plt.title(\"Proportion of Accidents by Hour in Top 20 Cities\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"City\")\n",
    "plt.xticks(ticks=range(0, 24), labels=[str(i) for i in range(24)])  # Ensure hour ticks are 0-23\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bdd008e06e9f4602b065c8cb0790e3ec",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Both heatmaps validate there is a trend across all the different cities and streets, justifying using these as dimensions for our core analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ba983cd794d04bd08ac97daa18f63755",
    "deepnote_cell_type": "code",
    "deepnote_output_height_limit_disabled": true,
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 687,
    "execution_start": 1745137315713,
    "source_hash": "45b156bb"
   },
   "outputs": [],
   "source": [
    "# group by city and day of week, count accidents\n",
    "df_cleaned['Day_of_Week'] = df_cleaned['Start_Time'].dt.day_name()\n",
    "city_day = df_cleaned.groupby(['City', 'Day_of_Week']).size().unstack(fill_value=0)\n",
    "\n",
    "# normalize row-wise to get proportions per city\n",
    "city_day_prop = city_day.div(city_day.sum(axis=1), axis=0)\n",
    "\n",
    "# reorder columns to match actual day order\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "city_day_prop = city_day_prop[ordered_days]\n",
    "\n",
    "# take top 20 cities by total accidents\n",
    "top20_cities = df_cleaned['City'].value_counts().head(20).index\n",
    "city_day_top20 = city_day_prop.loc[top20_cities]\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sb.heatmap(city_day_top20, cmap='YlGnBu', cbar_kws={'label': 'Proportion of Accidents'})\n",
    "plt.title(\"Proportion of Accidents by Day in Top 20 Cities\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"City\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ebddce4b10dc4dc5b53998b2bb0f8452",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "The day trends are relevant to the cities with largest number of accidents, validating our focus on it. \r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6d1e25a927e24b91abed7841db04e1dc",
    "deepnote_cell_type": "code",
    "deepnote_output_height_limit_disabled": true,
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 565,
    "execution_start": 1745137316463,
    "source_hash": "853066af"
   },
   "outputs": [],
   "source": [
    "# sort by total weekend proportion \n",
    "city_day_top20['Weekend'] = city_day_top20['Saturday'] + city_day_top20['Sunday']\n",
    "city_day_top20_sorted = city_day_top20.sort_values(by='Weekend', ascending=False).drop(columns='Weekend')\n",
    "\n",
    "# create annotation DataFrame with formatted strings\n",
    "annot = (city_day_top20_sorted * 100).round(1).astype(str) + '%'\n",
    "annot = annot.values  # Ensure it's a NumPy array\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sb.heatmap(city_day_top20_sorted, cmap='YlGnBu', cbar_kws={'label': 'Proportion of Accidents'}, \n",
    "            annot=annot, fmt='', annot_kws={\"size\": 9})\n",
    "plt.title(\"Proportion of Accidents by Day in Top 20 Cities (Sorted by Weekend Share)\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"City\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5dc3bac2ea7b459e9974a92cf9efdea5",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "The above also validates that the trend is seen across all cities. Bakersfield has the most even distribution, but even it shows some differentiation. We will continue to look at this trend in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f523104b216c462290b23691256e0f08",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "The below shows us the weather analysis. We wish to see if all the accidents in specific streets occur in adverse conditions. This will assist in classifying if the streets are dangerous only in adverse weather, or if they are dangerous just in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b1537d69531c454fb3dc8e58a1225eb4",
    "deepnote_cell_type": "code",
    "execution_context_id": "5fd2853d-870e-403c-a777-68a345b9e92d",
    "execution_millis": 872,
    "execution_start": 1745137317683,
    "source_hash": "58cd1976"
   },
   "outputs": [],
   "source": [
    "# create Weather_Category based on Is_Clear flag\n",
    "df['Weather_Category'] = df['Is_Clear'].apply(lambda x: 'Clear' if x == 1 else 'Not Clear')\n",
    "\n",
    "# get top 20 streets by accident count\n",
    "top_streets = df['Street'].value_counts().head(20).index\n",
    "\n",
    "# filter to those streets\n",
    "df_top_streets = df[df['Street'].isin(top_streets)].copy()\n",
    "\n",
    "# createountplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sb.countplot(\n",
    "    data=df_top_streets,\n",
    "    y='Street',\n",
    "    hue='Weather_Category',\n",
    "    order=top_streets,\n",
    "    palette=['#6baed6', '#ff6f61']  # Clear = blue, Not Clear = red\n",
    ")\n",
    "\n",
    "# add percentages to the bars\n",
    "# get counts grouped by street and weather\n",
    "counts = (\n",
    "    df_top_streets.groupby(['Street', 'Weather_Category'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# add annotation on bars\n",
    "for i, street in enumerate(top_streets):\n",
    "    total = counts.loc[street].sum()\n",
    "    clear_count = counts.loc[street].get('Clear', 0)\n",
    "    not_clear_count = counts.loc[street].get('Not Clear', 0)\n",
    "\n",
    "    # get bar coordinates and add text\n",
    "    bar_clear = ax.patches[i * 2]  # Each street has two bars (hue)\n",
    "    bar_not_clear = ax.patches[i * 2 + 1]\n",
    "\n",
    "plt.title(\"Accidents on Top 20 Streets by Weather Condition (Full Dataset)\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.ylabel(\"Street\")\n",
    "plt.legend(title=\"Weather\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc04f3423e524cafaf3e9287b70f38f4",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "We see that unclear weather contributes heavily to the occurrence of accidents, making it a key factor that we wish to consider. The different factors of weather can be analysed to make predictions. For streets that show more evenness, we can exclude them.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b855cae1b2d94abfae2f71f61abcd978",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "To conclude, our three dimensions of region, time+day and weather are all validated. We narrow down specifically to investigate cities and streets for region, peak hour trends and weekday-weekend splits for time+day, and indicators of adverse weather. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Machine Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have performed our EDA on the dataset, we understand a few key trends better. It is evident that there are a few key cities and regions that experience significantly higher volumes of accidents than others. We've also identified a trend throughout the day where accidents spike during peak hours and seem to be higher during the peak hours in the evening as opposed to the morning. \n",
    "\n",
    "Understanding these trends, we now want to apply our dataset into some Machine Learning applications. There were a few ideas that we considered as outputs from our algorithms.\n",
    "\n",
    "1. Binary classification on whether a condition would result in accidents\n",
    "2. Time series forecast to predict the rate of accidents in the next hour at a given hour, region and weather\n",
    "3. Severity prediction for an accident based on time, region and weather\n",
    "\n",
    "Unfortunately, as we are working with a positives-only dataset, it would be impossible to create an algorithm that could perform the binary classification we would want in 1 unless we merged our dataset with another. Furthermore, as seen above, only one unique value of severity of accident exists making it impossible to conduct classification on the severity. Thus, we will be going ahead to create algorithms for objectives 2 only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, let us tackle algorithm 2, predicting the rates of accidents in the upcoming hour based on the location, weather, time and day of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the dataset easier to work with for our models, let us first remove all unnecessary columns. We will also work with cities first as aggregating by city would be easier to work with for the localised prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'Start_Time',  # Time information\n",
    "    'City', #City\n",
    "    'Temperature(F)',  # Weather data\n",
    "    'Humidity(%)',  # Weather data\n",
    "    'Pressure(in)', #Pressure data\n",
    "    'Wind_Speed(mph)',  # Weather data\n",
    "    'Start_Hour',  # Extracted from Start_Time\n",
    "    'Day_of_Week',  # Extracted from Start_Time\n",
    "    'Is_Windy',\n",
    "    'Is_Stormy',\n",
    "    'Is_Rainy',\n",
    "    'Is_Foggy',\n",
    "    'Is_Snowy',\n",
    "    'Is_Clear'\n",
    "]\n",
    "\n",
    "# Drop all other columns\n",
    "df_algo1 = df_cleaned[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algo1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reducing the dataset to focus on our key predictors, we now need to create the target variable for predicting the rate of accidents in the next hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Floor to hour\n",
    "df_algo1['Start_Hourly'] = df_algo1['Start_Time'].dt.floor('H')\n",
    "\n",
    "# Step 2: Create \"Next Hour\"\n",
    "df_algo1['Next_Hourly'] = df_algo1['Start_Hourly'] + pd.Timedelta(hours=1)\n",
    "\n",
    "# Step 3: Count future accidents per city per hour\n",
    "city_hourly_counts = df_algo1.groupby(['City', 'Start_Hourly']).size().rename('Accident_Count')\n",
    "\n",
    "# Step 4: Shift within each city to get *next hour's* count\n",
    "city_hourly_next = city_hourly_counts.groupby(level=0).shift(-1).rename('Accident_Next_Hour')\n",
    "\n",
    "# Step 5: Combine the City/Hour index with the shifted counts\n",
    "target_df = pd.concat([city_hourly_counts, city_hourly_next], axis=1).reset_index()\n",
    "\n",
    "# Step 6: Merge back into main df\n",
    "df_algo1 = df_algo1.merge(target_df[['City', 'Start_Hourly', 'Accident_Next_Hour']],\n",
    "              on=['City', 'Start_Hourly'], how='left')\n",
    "\n",
    "\n",
    "print(df_algo1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algo1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make sure your time is rounded to the hour\n",
    "df_algo1['Start_Hourly'] = df_algo1['Start_Time'].dt.floor('H')\n",
    "\n",
    "# Step 2: Count number of accidents per city per hour\n",
    "city_hourly_counts = df_algo1.groupby(['City', 'Start_Hourly']).size().rename('Accident_Current_Hour')\n",
    "\n",
    "# Step 3: Create the lag (previous hour's accident count)\n",
    "city_hourly_counts_lag = city_hourly_counts.groupby(level=0).shift(1).rename('Accident_Prev_Hour')\n",
    "\n",
    "# Step 4: Combine the counts into a DataFrame for merging\n",
    "lag_features = pd.concat([city_hourly_counts, city_hourly_counts_lag], axis=1).reset_index()\n",
    "\n",
    "# Step 5: Merge lag features into main DataFrame\n",
    "df_algo1 = df_algo1.merge(lag_features, how='left', on=['City', 'Start_Hourly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is the cardinality of the City variable is very high, it would not be possible to convert it with One-Hot Encoding. Additionally, if label encoding is used, patterns about the distance between the numbers could be identified when in reality there is no pattern between the integers given to each city. Thus, we will use a different method called Target Encoding instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the target variable for each city\n",
    "city_target_mean = df_algo1.groupby('City')['Accident_Next_Hour'].mean()\n",
    "\n",
    "# Map the mean target value to the 'City' column\n",
    "df_algo1['City_Encoded'] = df_algo1['City'].map(city_target_mean)\n",
    "\n",
    "# Check the result\n",
    "print(df_algo1.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Day_of_Week currently exists as strings, we shall apply a simple ordinal encoding to allow it to be processed by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week_map = {\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Day_of_Week' column\n",
    "df_algo1['Day_of_Week'] = df_algo1['Day_of_Week'].map(day_of_week_map)\n",
    "\n",
    "# Check the result\n",
    "print(df_algo1[['Day_of_Week']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where target is NaN\n",
    "df_algo1 = df_algo1.dropna(subset=['Accident_Prev_Hour'])\n",
    "df_algo1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algo1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the hours of the day are cyclical, let's convert our start_hour to represent the cyclical nature instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of the day (0 to 23)\n",
    "df_algo1['Start_Hour'] = df_algo1['Start_Time'].dt.hour\n",
    "\n",
    "# Convert hour into sine and cosine to capture cyclic nature\n",
    "df_algo1['Hour_Sin'] = np.sin(2 * np.pi * df_algo1['Start_Hour'] / 24)\n",
    "df_algo1['Hour_Cos'] = np.cos(2 * np.pi * df_algo1['Start_Hour'] / 24)\n",
    "\n",
    "df_algo1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algo1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_algo1[['Temperature(F)', 'Humidity(%)', 'Wind_Speed(mph)', 'Pressure(in)', 'Day_of_Week', 'City_Encoded', 'Is_Windy',\n",
    "    'Is_Stormy',\n",
    "    'Is_Rainy',\n",
    "    'Is_Foggy',\n",
    "    'Is_Snowy',\n",
    "    'Is_Clear',\n",
    "    'Hour_Sin', 'Hour_Cos',\n",
    "    'Accident_Prev_Hour'] ]\n",
    "y = df_algo1['Accident_Next_Hour']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be testing 3 different algorithms to determine which will perform the best.\n",
    "\n",
    "1. Random Forest Regressor\n",
    "2. Gradient Boosting Regressor\n",
    "3. Linear Regression\n",
    "   \n",
    "We will define hyperparameter grids to tune the hyperparameters for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for Ridge Regression (regularized Linear Regression)\n",
    "lr_param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model, param_grid, X_train, y_train, X_test, y_test, n_iter=1):\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=n_iter, cv=2,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "    \n",
    "    with tqdm(total=n_iter, desc=f\"Training {model.__class__.__name__}\") as pbar:\n",
    "        search.fit(X_train, y_train)\n",
    "        pbar.update(n_iter)\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return best_model, mse, mae, r2, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Check and Handle NaN values in y_train ---\n",
    "# This section is added to fix the \"Input y contains NaN\" error\n",
    "print(\"--- NaN Handling ---\")\n",
    "initial_y_nans = y_train.isna().sum()\n",
    "# Handle case where y_train might be a DataFrame (multi-output)\n",
    "if isinstance(y_train, pd.DataFrame):\n",
    "    initial_y_nans = initial_y_nans.sum()\n",
    "print(f\"Number of NaNs initially in y_train: {initial_y_nans}\")\n",
    "\n",
    "if initial_y_nans > 0:\n",
    "    print(f\"Original shape X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    # Create a boolean mask for rows where y_train IS NOT NaN\n",
    "    # If y_train is a DataFrame, .all(axis=1) keeps rows where *all* outputs are non-NaN.\n",
    "    # Adjust to .any(axis=1) if needed, depending on your multi-output strategy.\n",
    "    if isinstance(y_train, pd.DataFrame):\n",
    "         mask = y_train.notna().all(axis=1)\n",
    "    else: # Assumes y_train is a Pandas Series\n",
    "         mask = y_train.notna()\n",
    "\n",
    "    # Apply mask to remove rows with NaN in y_train from BOTH X_train and y_train\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "\n",
    "    # Confirm removal\n",
    "    final_y_nans = y_train.isna().sum()\n",
    "    if isinstance(y_train, pd.DataFrame): final_y_nans = final_y_nans.sum()\n",
    "    print(f\"New shape after removing NaN y_train rows: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Number of NaNs remaining in y_train: {final_y_nans}\")\n",
    "else:\n",
    "    print(\"No NaNs found in y_train.\")\n",
    "print(\"--- End NaN Handling ---\")\n",
    "\n",
    "# --- Check for NaNs in X_train (Good Practice) ---\n",
    "# Note: Handling X_train NaNs (e.g., imputation) should ideally happen earlier.\n",
    "initial_x_nans = X_train.isna().sum().sum()\n",
    "print(f\"Total NaNs found in X_train: {initial_x_nans}\")\n",
    "if initial_x_nans > 0:\n",
    "    print(\"Warning: NaNs detected in X_train. Ensure they are handled appropriately (e.g., imputation).\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Initialise models ---\n",
    "# (Your original initialization)\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "lr_model = Ridge()\n",
    "\n",
    "# --- Train and collect metrics + predictions ---\n",
    "# Added n_iter parameter (adjust value as needed for RandomizedSearchCV)\n",
    "tuning_iterations = 10 # Example: test 10 random parameter combinations per model\n",
    "\n",
    "# Ensure your tune_model function accepts and uses the n_iter argument\n",
    "rf_best_model, rf_mse, rf_mae, rf_r2, rf_pred = tune_model(rf_model, rf_param_grid, X_train, y_train, X_test, y_test, n_iter=tuning_iterations)\n",
    "gb_best_model, gb_mse, gb_mae, gb_r2, gb_pred = tune_model(gb_model, gb_param_grid, X_train, y_train, X_test, y_test, n_iter=tuning_iterations)\n",
    "# Note: RandomizedSearch might be less common for Ridge, but using for consistency.\n",
    "# Adjust n_iter or use GridSearchCV via tune_model if lr_param_grid is small.\n",
    "lr_best_model, lr_mse, lr_mae, lr_r2, lr_pred = tune_model(lr_model, lr_param_grid, X_train, y_train, X_test, y_test, n_iter=tuning_iterations)\n",
    "\n",
    "# --- Create a results table ---\n",
    "# Added checks to ensure models trained successfully before adding results\n",
    "# (Assumes tune_model returns None or np.inf/nan on failure - adjust if needed)\n",
    "results = {}\n",
    "if rf_best_model is not None and np.isfinite(rf_mse):\n",
    "     results['Random Forest'] = {'MSE': rf_mse, 'MAE': rf_mae, 'R': rf_r2}\n",
    "if gb_best_model is not None and np.isfinite(gb_mse):\n",
    "     results['Gradient Boosting'] = {'MSE': gb_mse, 'MAE': gb_mae, 'R': gb_r2}\n",
    "if lr_best_model is not None and np.isfinite(lr_mse):\n",
    "    # Changed key slightly for clarity\n",
    "    results['Linear Regression (Ridge)'] = {'MSE': lr_mse, 'MAE': lr_mae, 'R': lr_r2}\n",
    "\n",
    "# Only create and print DataFrame if results dictionary is not empty\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(results_df.round(4))\n",
    "else:\n",
    "    print(\"\\nNo models trained successfully or yielded valid metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs Predicted\n",
    "def plot_actual_vs_pred(y_true, y_pred, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3, edgecolor='k')\n",
    "    plt.xlabel(\"Actual Accident Count\")\n",
    "    plt.ylabel(\"Predicted Accident Count\")\n",
    "    plt.title(f\"Actual vs Predicted: {title}\")\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Residuals\n",
    "def plot_residuals(y_true, y_pred, title):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(residuals, bins=50, alpha=0.7, color='purple')\n",
    "    plt.title(f\"Residuals: {title}\")\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Run for each model\n",
    "plot_actual_vs_pred(y_test, rf_pred, \"Random Forest\")\n",
    "plot_residuals(y_test, rf_pred, \"Random Forest\")\n",
    "\n",
    "plot_actual_vs_pred(y_test, gb_pred, \"Gradient Boosting\")\n",
    "plot_residuals(y_test, gb_pred, \"Gradient Boosting\")\n",
    "\n",
    "plot_actual_vs_pred(y_test, lr_pred, \"Linear Regression\")\n",
    "plot_residuals(y_test, lr_pred, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_algo1['Accident_Next_Hour'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_accidents(x):\n",
    "    if x == 0:\n",
    "        return 'none'\n",
    "    elif x <= 2:\n",
    "        return 'low'\n",
    "    elif x <= 5:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df_algo1['Accident_Class'] = df_algo1['Accident_Next_Hour'].apply(categorize_accidents)\n",
    "df_algo1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    'Temperature(F)', 'Humidity(%)', 'Wind_Speed(mph)', 'Pressure(in)',\n",
    "    'City_Encoded', 'Is_Windy', 'Is_Stormy', 'Is_Rainy',\n",
    "    'Is_Foggy', 'Is_Snowy', 'Is_Clear',\n",
    "    'Hour_Sin', 'Hour_Cos', 'Accident_Prev_Hour'\n",
    "]\n",
    "\n",
    "# Encode target classes\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df_algo1['Accident_Class'])\n",
    "# Define features and target\n",
    "X = df_algo1[feature_cols]\n",
    "y = y_encoded\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(\"Before SMOTE:\", y_train.value_counts())\n",
    "# print(\"After SMOTE:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Training {name}...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Save overall accuracy and macro F1\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1 (macro)\": f1_score(y_test, y_pred, average='macro')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns  # assuming X was the feature DataFrame used\n",
    "\n",
    "# Access the already-trained Random Forest model\n",
    "rf_model = models['Random Forest']\n",
    "\n",
    "# Check if the model supports feature importances\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    importances = rf_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame of features and their importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Display top 10\n",
    "    print(\" Top 10 Most Important Features:\\n\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'][:10][::-1], importance_df['Importance'][:10][::-1])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Top 10 Feature Importances - Random Forest\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\" This model does not support feature importances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data-Driven Insights </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classification model, we understand that City has the highest prediction importance at 0.211, followed by Humidity (0.151) , Pressure (0.149), Temperature (0.148), Windspeed (0.112), Accident In Previous Hour (0.099) and Hour (0.058). \n",
    "\n",
    "This supports that Los Angeles, Sacramento, San Diego and San Jose would have a significantly higher possibility of accidents relative to other cities. The number of accidents drops sharply after these four cities, making it harder to make definitive predictions for the other cities. \n",
    "\n",
    "Next, the series of climate-related metrics that are all interrelated, allude to adverse weather. High humidity is possibly an indicator of precipitation, but the correlation to temperature possibly links to driver comfort as well. \n",
    "\n",
    "Accidents in Previous Hour shows high clustering of accidents, and Hour supports our peak hour hypothesis. \n",
    "\n",
    "Therefore, to interpret the outputs exactly as an example, we would be at our highest caution when driving in Los Angeles at 5PM on a Friday, especially when it is raining in the summer and there have been other accidents in the vicinity.\n",
    "\n",
    "The model also warns that it shows strong predictive accuracy for low and moderate severity accidents, especially severity 1 and 2  with recall scores of 0.83 and 0.74 respectively. However, it struggles to distinguish between severity 3 and 4 accidents due to overlapping environmental features. This is evident in the confusion matrix where 47% of severity 4 accidents are misclassified. \n",
    "\n",
    "Therefore, we have to remember the caveat that while these predictions will help us with general accidents, freak catastrophic accidents can still very much occur in any situation, without following these trends. This keeps us alert and prevents any complacency in driving. \n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "8ce5ec165fb0496db0cbacc88aff9ddf",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
